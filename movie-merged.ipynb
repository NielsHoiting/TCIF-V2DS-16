{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB\n",
    "This notebook contains the first few steps of the data science pipeline on a dataset containing movies.\n",
    "\n",
    "## Group\n",
    "V2H-Groep 1: Films (IMDB)\n",
    "- Niels Hoiting\n",
    "- Jari Oostrom\n",
    "- Yusuf Syakur\n",
    "\n",
    "## Research questions\n",
    "1. [What is the correlation between the gender of actors and the popularity of the movie. How does this change overtime?](#What-is-the-correlation-between-the-gender-of-the-cast-and-the-popularity-of-the-movie.)\n",
    "2. [What happens if we cluster this dataset, leaving out the genre variable?](#What-happens-if-we-cluster-this-dataset,-leaving-out-the-genre-variable?)\n",
    "3. [To what extend can you predict the gross of a movie based on its popularity on Facebook and IMDB?](#To-what-extend-can-you-predict-the-gross-of-a-movie-based-on-its-popularity-on-Facebook-and-IMDB?)\n",
    "\n",
    "## Dataset\n",
    "Movie information with duration, genres, languages, country, budget and gross;\n",
    "likes on facebook for director, main cast, total cast en the movie itself;\n",
    "score on IMDB and reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data collection\n",
    "Import needed libraries. The dataset is already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "df_movies = pd.read_csv('movie.csv')\n",
    "df_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data processing (Data munging)\n",
    "Look at the current dataframe and their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_movies.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_movies.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current column order does not make sense. Order them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_movies = df_movies[['movie_imdb_link', 'movie_title', 'imdb_score', 'title_year', 'director_name', 'director_facebook_likes', 'actor_1_name',\n",
    "                      'actor_1_facebook_likes', 'actor_2_name', 'actor_2_facebook_likes', 'actor_3_name', 'actor_3_facebook_likes',\n",
    "                      'cast_total_facebook_likes', 'movie_facebook_likes', 'genres', 'budget', 'gross', 'country', 'language',\n",
    "                      'num_critic_for_reviews', 'num_user_for_reviews', 'num_voted_users', 'plot_keywords', 'color', 'content_rating',\n",
    "                      'duration', 'aspect_ratio', 'facenumber_in_poster']]\n",
    "df_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Cleaning\n",
    "\n",
    "Drop overall duplicates first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Before removing duplicates', df_movies.shape)\n",
    "df_movies = df_movies.drop_duplicates()\n",
    "print('After removing duplicates:', df_movies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 movie_imdb_link\n",
    "\n",
    "The movie_imdb_link duplicates only differ on a few columns like likes and votes. Extract the unique identifier from the URL and remove these duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat(gby_result for _, gby_result in df_movies.groupby(\"movie_imdb_link\") if len(gby_result) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_movies['movie_imdb_link'] = df_movies['movie_imdb_link'].str.extract(r'(?<=title\\/)(.*)(?=\\/\\?)', expand=False)\n",
    "print('Length before removing duplicates', df_movies.shape)\n",
    "df_movies = df_movies.drop_duplicates(subset='movie_imdb_link')\n",
    "print('Length after removing duplicates:',df_movies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 movie_title\n",
    "\n",
    "Strip whitespaces from both ends for the title. Duplicate movie_title rows might be a remake or a reboot of the movie. Leave them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_movies['movie_title'] = df_movies['movie_title'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 title_year\n",
    "Rows that have NaN for title_year are series/reviews, not movies. We won't need these for our analysis. CHange title_year to DateTime64 for time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_movies.loc[df_movies['title_year'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Length before removing NaN for title_year:', df_movies.shape)\n",
    "df_movies = df_movies.drop(df_movies.loc[df_movies['title_year'].isnull()].index)\n",
    "print('Length after removing NaN for title_year:', df_movies.shape)\n",
    "df_movies['title_year'] = pd.to_datetime(df_movies['title_year'], format='%Y', errors='coerce')\n",
    "df_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 actor_1_name\n",
    "Rows that have NaN for actor_1_name are documentaries, not movies. Remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_movies.loc[df_movies['actor_1_name'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Length before removing NaN for actor_1_name:', df_movies.shape)\n",
    "df_movies = df_movies.drop(df_movies.loc[df_movies['actor_1_name'].isnull()].index)\n",
    "print('Length after removing NaN for actor_1_name:', df_movies.shape)\n",
    "df_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 genres\n",
    "\n",
    "Genres are split with an '|' delimeter. In total there are 28 unique genres. There are no NaN values. Split them and give them an own boolean column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list_genres = list(set(itertools.chain.from_iterable(df_movies.genres.str.split('|'))))\n",
    "print(list_genres)\n",
    "\n",
    "def add_genre(df, genre):\n",
    "    genreConcat = 'genre_' + genre\n",
    "    df_copy = df.copy()\n",
    "    df_copy[genreConcat] = df_copy['genres'].str.contains(pat = genre)\n",
    "    return df_copy\n",
    "\n",
    "for genre in list_genres:\n",
    "    df_movies = add_genre(df_movies, genre)\n",
    "\n",
    "df_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 plot_keywords\n",
    "Remove '|' delimeter to able to use text mining (if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_movies['plot_keywords'] = df_movies['plot_keywords'].str.replace('|', ' ')\n",
    "df_movies['plot_keywords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 content_rating\n",
    "Replace NaN and 'Unrated' with 'Not Rated'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_movies['content_rating'].unique())\n",
    "\n",
    "df_movies['content_rating'] = df_movies['content_rating'].str.replace('Unrated', 'Not Rated')\n",
    "df_movies['content_rating'] = df_movies['content_rating'].fillna(value='Not Rated')\n",
    "\n",
    "print(df_movies['content_rating'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 color\n",
    "All rows with NaN on color are released after 1990. Assume color is used (available since 1950s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_movies['color'] = df_movies['color'].fillna(value='Color')\n",
    "df_movies['color'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Remove unimportant NaN's\n",
    "\n",
    "Remove rows that have columns with NaN values. These NaN values can't be filled in by a 'default' value. Leave budget and gross (might turn out to be too much data loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Length before removing NaNs', len(df_movies))\n",
    "\n",
    "cols_to_ignore = ['movie_imdb_link', 'budget']\n",
    "df_budget_gross = df_movies[cols_to_ignore]\n",
    "df_movies = df_movies.drop(['budget'], axis=1)\n",
    "\n",
    "df_movies = df_movies.dropna()\n",
    "\n",
    "print('Length after removing NaNs', len(df_movies))\n",
    "\n",
    "df_movies = df_movies.join(df_budget_gross.set_index('movie_imdb_link'), on='movie_imdb_link')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 Change to int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_movies = df_movies.astype({'director_facebook_likes': 'int64',\n",
    "                            'actor_1_facebook_likes': 'int64',\n",
    "                            'actor_2_facebook_likes': 'int64',\n",
    "                            'actor_3_facebook_likes': 'int64',\n",
    "                            'cast_total_facebook_likes': 'int64',\n",
    "                            'num_critic_for_reviews': 'int64',\n",
    "                            'num_user_for_reviews': 'int64',\n",
    "                            'num_voted_users': 'int64',\n",
    "                            'duration': 'int64',\n",
    "                            'facenumber_in_poster': 'int64',\n",
    "                              'gross': 'float64'})\n",
    "\n",
    "df_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the correlation between the gender of the cast and the popularity of the movie.\n",
    "\n",
    "In order to find a correlation between gender of actors and popularity we need to define what a 'popular' movie is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imports \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "\n",
    "import holoviews as hv\n",
    "hv.extension('matplotlib')\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df_gender = df_movies.copy()\n",
    "\n",
    "# Place genres into a array\n",
    "df_gender.genres = df_gender.genres.str.split(pat = \"|\")\n",
    "\n",
    "# Get the unique genres\n",
    "movie_genres = df_gender.genres.explode().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## What is popularity\n",
    "\n",
    "There are a couple of fields that can indicate popularity\n",
    "- `movie_facebook_likes`\n",
    "- `num_critic_for_reviews`\n",
    "- `gross`\n",
    "- `imdb_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Total shape\", df_gender.shape)\n",
    "print(\"After dropping NAs\", df_gender.dropna().shape)\n",
    "\n",
    "print(\"movie_facebook_likes > 0\", df_gender[df_gender.movie_facebook_likes > 0].shape)\n",
    "print(\"num_critic_for_reviews > 0\", df_gender[df_gender.num_critic_for_reviews > 0].shape)\n",
    "print(\"gross > 0\", df_gender[df_gender.gross > 0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Cast and gender\n",
    "Our current dataset does not contain data about the gender. We will join the dataset with another dataset from the same source: The Movie Database. First we will need to remove the movie titles trailing spaces.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fixing trailing characters\n",
    "df_gender['movie_title'] = df_gender.movie_title.str.replace('[^\\x00-\\x7F]','')\n",
    "df_gender['year'] = df_gender.title_year.apply(lambda m: m.year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new dataset contains 4 columns: `movie_id`, `movie_title`, `cast` and `crew`. We are interted in the `cast` column. The `cast` column contains an array with the cast of the movie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# A credits dataset we can join with our movie dataset.\n",
    "df_credits = pd.read_csv('tmdb_5000_credits.csv')\n",
    "df_credits = df_credits.rename(columns={'title': 'movie_title'})\n",
    "\n",
    "df_credits.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Inner joining based on the title of the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Joining the two datasets\n",
    "movie_with_cast = pd.merge(df_gender, df_credits, how=\"inner\", on=\"movie_title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The credits dataset adds a column called `cast`. This column contains an array with objects. Each object represents a actor/actress.\n",
    "The genders of the actors are stored in the `gender` field in the object. Possible three possible values are:\n",
    "\n",
    "|Value   | Gender  |\n",
    "|---|---|\n",
    "| 0  | Unknown  |\n",
    "| 1  | Female  |\n",
    "| 2  | Male  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ideally we have one value that represents the share of males and females within the cast of a movie. The first step toward this value is creating a vector for each possible value. After that a column for each gender (male, female and unknown) is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cast is a nested field, this function will return the gender for the given cast and name.\n",
    "def actor_to_gender(cast):\n",
    "    cast = json.loads(cast)\n",
    "    ratio = [0, 0, 0]\n",
    "    for actor in cast:\n",
    "        ratio[actor['gender']] += 1\n",
    "    return ratio\n",
    "\n",
    "movie_with_cast['gender_ratio'] = movie_with_cast.apply(lambda movie: actor_to_gender(movie.cast), axis=1)\n",
    "movie_with_cast['unknown_actors'] = movie_with_cast.apply(lambda movie: movie['gender_ratio'][0], axis=1)\n",
    "movie_with_cast['female_actors'] = movie_with_cast.apply(lambda movie: movie['gender_ratio'][1], axis=1)\n",
    "movie_with_cast['male_actors'] = movie_with_cast.apply(lambda movie: movie['gender_ratio'][2], axis=1)\n",
    "movie_with_cast['total_known_actors'] = movie_with_cast.female_actors + movie_with_cast.male_actors\n",
    "movie_with_cast[['movie_title', 'gender_ratio', 'unknown_actors', 'female_actors', 'male_actors', 'total_known_actors']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the histograms illustrated below we can see that the most movies between 3 till 15 male actors and 3 till 7 female actresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "male = movie_with_cast.male_actors\n",
    "plt.hist(male, bins=range(0, 50, 1))\n",
    "plt.title(\"Frequency male\")\n",
    "plt.show()\n",
    "\n",
    "female = movie_with_cast.female_actors\n",
    "plt.title(\"Frequency female\")\n",
    "plt.hist(female, bins=range(0, 50, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In order to find a correlation between gender and popularity we need a numeric value that represents _gender_. For this we take a ratio. By dividing the `male_actors` by the sum of `female_actors` and `male_actors` we get an ratio representing the distribution of gender. Where a low number means relatively more female actors and a high number more male actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "movie_with_cast['ratio'] = movie_with_cast.male_actors / (movie_with_cast.male_actors + movie_with_cast.female_actors)\n",
    "movie_with_cast[['movie_title', 'ratio']].head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to compare similar movies with eachother we add one more filter. The total known actors should be higher than 20. This probably results in a dataset with higher grossing movies."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "movie_with_cast[['ratio', 'gender_ratio', 'imdb_score', 'gross', 'movie_facebook_likes', 'num_critic_for_reviews', 'male_actors' , 'female_actors', 'total_known_actors']].describe()\n",
    "filtered = movie_with_cast[movie_with_cast.total_known_actors >= 20]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As shown in the scatterplot it seems there is a light linear correlation between the two."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.regplot(x=\"ratio\", y=\"imdb_score\", data=filtered);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The degree of correlation between `imdb_score` and `ratio` can be measured by _Pearson correlation coefficient_. -1 and 1 would mean a high correlation. 0 would mean no correlation.\n",
    "\n",
    "A coefficient of `0.26653` is not very high but it means there is a small correlation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered[['ratio', 'imdb_score']].corr(method='pearson')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next up creating a linear regression model for determining the coefficient of determination (r2). The coefficient is very low. This seems logical based on the large amount of outliers shown in the scatterplot."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = filtered[['ratio']]\n",
    "y = filtered['imdb_score']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also want to know how the correlation changes overtime. This is illustrated in the graph below. The x axis represents time the y axis the _Pearson correlation coefficient_. The graph does not indicate some kind of trend."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr = filtered[['year', 'ratio', 'imdb_score']].groupby(['year']).corr()\n",
    "corr = corr.dropna()\n",
    "corr = corr.iloc[0::2,-1]\n",
    "corr.index = corr.index.get_level_values(0)\n",
    "\n",
    "corr.plot();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Genres can also change the correlation between success and the gender ratio. The interactive graph below illustrates a scatterplot based on the genre. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_genre(genre, **kwargs):\n",
    "    #filter\n",
    "    if genre == 'All':\n",
    "        genre_filtered = filtered\n",
    "    else:\n",
    "        genre_filtered = filtered[filtered.apply(lambda m: genre in m.genres, axis=1)]\n",
    "    scat = hv.Scatter(genre_filtered[['ratio', 'imdb_score']])\n",
    "    \n",
    "    #Pearson correlation coefficient\n",
    "    r = genre_filtered.corr(method='pearson')['imdb_score']['ratio']\n",
    "    \n",
    "    # Create title with genre and options.\n",
    "    (scat).opts(title='Genre: ' + genre + \" | Pearson correlation coefficient: %.3f\" % r)\n",
    "    \n",
    "    return scat\n",
    "\n",
    "genres = movie_genres\n",
    "genres = np.append(genres, 'All')\n",
    "dmap = hv.DynamicMap(load_genre, kdims='Genre').redim.values(Genre=genres)\n",
    "dmap.opts(height=500, width=500)\n",
    "dmap"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notable genres:\n",
    "\n",
    "| Genre | Coefficient|\n",
    "|--|--|\n",
    "| Western | 0.716 |\n",
    "| Music | -0.026 |\n",
    "| Musical | 0.509 |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## What happens if we cluster this dataset, leaving out the genre variable?\n",
    "The goal is to see if groups that were formed as a result of clustering, make any sense when comparing it to genre. Lets make a subset of the dataframe picking up all the variables except the boolean categorical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare similar movies with eachother we add one more filter. The total known actors should be higher than 20. This probably results in a dataset with higher grossing movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_with_cast[['ratio', 'gender_ratio', 'imdb_score', 'gross', 'movie_facebook_likes', 'num_critic_for_reviews', 'male_actors' , 'female_actors', 'total_known_actors']].describe()\n",
    "filtered = movie_with_cast[movie_with_cast.total_known_actors >= 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the scatterplot it seems there is a light linear correlation between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.regplot(x=\"ratio\", y=\"imdb_score\", data=filtered);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The degree of correlation between `imdb_score` and `ratio` can be measured by _Pearson correlation coefficient_. -1 and 1 would mean a high correlation. 0 would mean no correlation.\n",
    "\n",
    "A coefficient of `0.26653` is not very high but it means there is a small correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered[['ratio', 'imdb_score']].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up creating a linear regression model for determining the coefficient of determination (r2). The coefficient is very low. This seems logical based on the large amount of outliers shown in the scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = filtered[['ratio']]\n",
    "y = filtered['imdb_score']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to know how the correlation changes overtime. This is illustrated in the graph below. The x axis represents time the y axis the _Pearson correlation coefficient_. The graph does not indicate some kind of trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = filtered[['year', 'ratio', 'imdb_score']].groupby(['year']).corr()\n",
    "corr = corr.dropna()\n",
    "corr = corr.iloc[0::2,-1]\n",
    "corr.index = corr.index.get_level_values(0)\n",
    "\n",
    "corr.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genres can also change the correlation between success and the gender ratio. The interactive graph below illustrates a scatterplot based on the genre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_genre(genre, **kwargs):\n",
    "    #filter\n",
    "    if genre == 'All':\n",
    "        genre_filtered = filtered\n",
    "    else:\n",
    "        genre_filtered = filtered[filtered.apply(lambda m: genre in m.genres, axis=1)]\n",
    "    scat = hv.Scatter(genre_filtered[['ratio', 'imdb_score']])\n",
    "    \n",
    "    #Pearson correlation coefficient\n",
    "    r = genre_filtered.corr(method='pearson')['imdb_score']['ratio']\n",
    "    \n",
    "    # Create title with genre and options.\n",
    "    (scat).opts(title='Genre: ' + genre + \" | Pearson correlation coefficient: %.3f\" % r);\n",
    "    \n",
    "    return scat\n",
    "\n",
    "genres = movie_genres\n",
    "genres = np.append(genres, 'All')\n",
    "dmap = hv.DynamicMap(load_genre, kdims='Genre').redim.values(Genre=genres)\n",
    "dmap.opts(height=500, width=500)\n",
    "dmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notable genres:\n",
    "\n",
    "| Genre | Coefficient|\n",
    "|--|--|\n",
    "| Western | 0.716 |\n",
    "| Music | -0.026 |\n",
    "| Musical | 0.509 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens if we cluster this dataset, leaving out the genre variable?\n",
    "The goal is to see if groups that were formed as a result of clustering, make any sense when comparing it to genre. Lets make a subset of the dataframe picking up all the variables except the boolean categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_nogenre = df_movies[['movie_imdb_link', 'movie_title', 'genres', 'imdb_score', 'title_year',\n",
    "       'director_name', 'director_facebook_likes', 'actor_1_name',\n",
    "       'actor_1_facebook_likes', 'actor_2_name', 'actor_2_facebook_likes',\n",
    "       'actor_3_name', 'actor_3_facebook_likes', 'cast_total_facebook_likes',\n",
    "       'movie_facebook_likes', 'country', 'language',\n",
    "       'num_critic_for_reviews', 'num_user_for_reviews', 'num_voted_users',\n",
    "       'plot_keywords', 'color', 'content_rating', 'duration', 'aspect_ratio',\n",
    "       'facenumber_in_poster', 'budget', 'gross']]\n",
    "df_nogenre.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "\n",
    "See if clustering with k-Means using a few combinations of numerical values will create easy-to-understand groups. There are alot of categorical variables in this set, these ones will have to drop because k-Means can only use numerical variables. There is an algorithm that is similar to k-Means that is based on categorical variables. This algorithm is called k-Modes. The most important categorical variables which should indicate genre's/similar groups are actors and directors. Let's see if we can use k-Modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Unique actors on 1: ', df_nogenre['actor_1_name'].unique().size)\n",
    "print('Unique actors on 2: ', df_nogenre['actor_2_name'].unique().size)\n",
    "print('Unique actors on 3: ', df_nogenre['actor_3_name'].unique().size)\n",
    "print('Unique directors: ', df_nogenre['director_name'].unique().size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important categorical variables have many unique values in comparison with the amount of records. Therefore we will not use the k-Modes algorithm. Since we have facebook likes for each director and actor, we can use these numerical values for the k-Means algorithm. Let's see a sample of those likes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_nogenre.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, that is strange. A highly appraised director such as James Cameron has zero facebook likes. Visiting his facebook page, this is not the case. Avatar is not the only one that James Cameron has directed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_nogenre.loc[df_nogenre['director_name'] == 'James Cameron']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of his great works contain zero facebook likes as a director. Something went probably wrong during the retrieval of this dataset. \n",
    "\n",
    "A first option would be to scrape all director facebook likes properly where director_facebook_likes == 0. But this means that you would get a new timestamp of data retrieval, resulting in doing harm to the integrity of the data. Let's not do that.\n",
    "\n",
    "Another option would be to scrape ALL of the directors facebook likes, but that is out of scope for this research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_nogenre.loc[df_nogenre['director_facebook_likes'] == 0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of rows with directors having zero facebook likes is 745. Dropping these would also mean dropping strong movies such as Titanic. Let's leave this variable for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_nogenre.loc[df_nogenre['actor_1_facebook_likes'] == 0].shape[0])\n",
    "print(df_nogenre.loc[df_nogenre['actor_2_facebook_likes'] == 0].shape[0])\n",
    "print(df_nogenre.loc[df_nogenre['actor_3_facebook_likes'] == 0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of rows with actors having zero facebook likes totals around 80. We can drop these rows and keep the rest for our k-Means model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Rows before:', df_nogenre.shape[0])\n",
    "df_nogenre.drop(df_nogenre.loc[df_nogenre['actor_1_facebook_likes'] == 0].index, inplace=True)\n",
    "df_nogenre.drop(df_nogenre.loc[df_nogenre['actor_2_facebook_likes'] == 0].index, inplace=True)\n",
    "df_nogenre.drop(df_nogenre.loc[df_nogenre['actor_3_facebook_likes'] == 0].index, inplace=True)\n",
    "print('Rows left:', df_nogenre.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column cast_total_facebook_likes looks like a summation of all 3 actor's facebook likes. Let's test this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_nogenre[['actor_1_facebook_likes', 'actor_2_facebook_likes', 'actor_3_facebook_likes']].sum(1)\n",
    "boolean_arr = np.where(df_nogenre[['actor_1_facebook_likes', 'actor_2_facebook_likes', 'actor_3_facebook_likes']].sum(1) == df_nogenre['cast_total_facebook_likes']\n",
    "                     , 'True', 'False')\n",
    "truefalse, amount = np.unique(boolean_arr, return_counts=True)\n",
    "np.asarray((truefalse, amount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently this is false for most of the rows. This means this column is not a result of 2 columns combined, which would interfere with creating the kMeans clusters. Let's keep this column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we noted that the data might be invalid, let's see how many rows we have for each of the remaining columns being on 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_nogenre.loc[df_nogenre['movie_facebook_likes'] == 0].shape[0])\n",
    "print(df_nogenre.loc[df_nogenre['num_critic_for_reviews'] == 0].shape[0])\n",
    "print(df_nogenre.loc[df_nogenre['num_user_for_reviews'] == 0].shape[0])\n",
    "print(df_nogenre.loc[df_nogenre['num_voted_users'] == 0].shape[0])\n",
    "print(df_nogenre.loc[df_nogenre['facenumber_in_poster'] == 0].shape[0])\n",
    "print(df_nogenre.loc[df_nogenre['budget'] == 0].shape[0])\n",
    "print(df_nogenre.loc[df_nogenre['gross'] == 0].shape[0])\n",
    "print(df_nogenre.loc[df_nogenre['cast_total_facebook_likes'] == 0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The amount of rows with movies having zero facebook likes is 1994. This makes sense because the dataset contains data from movies made in 1934 till 2016. Facebook was founded in 2004. Let's not use this column.\n",
    "\n",
    "Facenumber in poster having zero makes sense, because it can contain posters not having a face on it, for example alot of thriller movies.\n",
    "\n",
    "Since we are going with k-Means algorithm, let's drop all categorical variables as well, except for the identifiers (movie_imdb_link, movie_title, genres). We can also skip the mentioned director_facebook_likes and movie_facebook_likes column due to it's data being invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_nocat = df_nogenre[['movie_imdb_link', 'movie_title', 'genres', 'imdb_score', 'title_year', 'actor_1_facebook_likes', 'actor_2_facebook_likes',\n",
    "       'actor_3_facebook_likes', 'cast_total_facebook_likes', 'num_critic_for_reviews', 'num_user_for_reviews', \n",
    "       'num_voted_users', 'duration', 'aspect_ratio', 'facenumber_in_poster', 'budget', 'gross']]\n",
    "df_nocat.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because k-Means works with using the mean of a cluster, a date variable would make no sense. We can transform this variable to a numerical value. Since this dataset only contain year numbers, we only need to extract the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_nocat['title_year'] = pd.DatetimeIndex(df_nocat['title_year']).year\n",
    "df_nocat['title_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the results of k-Means, let's use the Z-Score form of standardization (as opposed to min-max or decimal normalization) for all of the variables. We can only standardize if there are no NaN values, so we should drop them. The data is now ready to be clustered using the k-Means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_nocat = df_nocat.dropna()\n",
    "df_nocat[['imdb_score']] = (df_nocat[['imdb_score']]-df_nocat[['imdb_score']].mean())/df_nocat[['imdb_score']].std()\n",
    "df_nocat[['actor_1_facebook_likes']] = (df_nocat[['actor_1_facebook_likes']]-df_nocat[['actor_1_facebook_likes']].mean())/df_nocat[['actor_1_facebook_likes']].std()\n",
    "df_nocat[['actor_2_facebook_likes']] = (df_nocat[['actor_2_facebook_likes']]-df_nocat[['actor_2_facebook_likes']].mean())/df_nocat[['actor_2_facebook_likes']].std()\n",
    "df_nocat[['actor_3_facebook_likes']] = (df_nocat[['actor_3_facebook_likes']]-df_nocat[['actor_3_facebook_likes']].mean())/df_nocat[['actor_3_facebook_likes']].std()\n",
    "df_nocat[['cast_total_facebook_likes']] = (df_nocat[['cast_total_facebook_likes']]-df_nocat[['cast_total_facebook_likes']].mean())/df_nocat[['cast_total_facebook_likes']].std()\n",
    "df_nocat[['num_critic_for_reviews']] = (df_nocat[['num_critic_for_reviews']]-df_nocat[['num_critic_for_reviews']].mean())/df_nocat[['num_critic_for_reviews']].std()\n",
    "df_nocat[['num_user_for_reviews']] = (df_nocat[['num_user_for_reviews']]-df_nocat[['num_user_for_reviews']].mean())/df_nocat[['num_user_for_reviews']].std()\n",
    "df_nocat[['num_voted_users']] = (df_nocat[['num_voted_users']]-df_nocat[['num_voted_users']].mean())/df_nocat[['num_voted_users']].std()\n",
    "df_nocat[['duration']] = (df_nocat[['duration']]-df_nocat[['duration']].mean())/df_nocat[['duration']].std()\n",
    "df_nocat[['aspect_ratio']] = (df_nocat[['aspect_ratio']]-df_nocat[['aspect_ratio']].mean())/df_nocat[['aspect_ratio']].std()\n",
    "df_nocat[['facenumber_in_poster']] = (df_nocat[['facenumber_in_poster']]-df_nocat[['facenumber_in_poster']].mean())/df_nocat[['facenumber_in_poster']].std()\n",
    "df_nocat[['budget']] = (df_nocat[['budget']]-df_nocat[['budget']].mean())/df_nocat[['budget']].std()\n",
    "df_nocat[['gross']] = (df_nocat[['gross']]-df_nocat[['gross']].mean())/df_nocat[['gross']].std()\n",
    "df_nocat[['title_year']] = (df_nocat[['title_year']]-df_nocat[['title_year']].mean())/df_nocat[['title_year']].std()\n",
    "df_nocat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to decide on a heuristic to evaluate if the clustered groups have any connection with the genre of a movie. The total amount of genres in this dataset is 28. We can say that if in every cluster there is a certain genre for >60% of that cluster, then it's safe to say that the clustering made sense (success). If it fails, then the clustering failed to make groups split by genre. In essence, we are actually testing the classification of the clustering by using the genre variable.\n",
    "\n",
    "We need to define a function that will let us choose the hyperparameters for the model building. We are only going to toy with the amount of clusters, the list of genres and which variables will be picked up for clustering. The function will print a tuple containing the cluster, percentage of the genre and the genre itself if it passes the heuristic.\n",
    "\n",
    "To start lets create a function that prints out a commonly found genre per cluster. We set the n_cluster to 28 (amount of genre), heuristic percentage to 60% and pick up all numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_cluster = 28\n",
    "heuristic_perc = 60\n",
    "chosen_genres = list(set(itertools.chain.from_iterable(df_nocat.genres.str.split('|'))))\n",
    "chosen_var = df_nocat.iloc[:,3:df_nocat.shape[1]].columns\n",
    "\n",
    "def sum_per_cluster(df, list_var, list_genres, n_clsters, heuristic_percentage):\n",
    "    print('Cluster\\tPercentage\\tGenre')\n",
    "    allMeans = KMeans(n_clusters=n_clsters, random_state=0).fit(df.loc[:,list_var])\n",
    "    df['Clusters'] = allMeans.predict(df.loc[:,list_var])\n",
    "    for cluster in range(0,n_cluster):\n",
    "        for genre in list_genres:\n",
    "            percentage = (df.loc[df['Clusters'] == cluster].genres.str.contains(genre).sum() / df.loc[df['Clusters'] == cluster].shape[0]) * 100\n",
    "            if percentage >= heuristic_percentage:\n",
    "                print(cluster, '\\t', np.around(percentage, decimals=2), '\\t\\t', genre)\n",
    "                \n",
    "sum_per_cluster(df_nocat, chosen_var, chosen_genres, n_cluster, heuristic_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to note about these results. There are a few clusters that contain 100% of a certain genre. This might seem ideal, but it could also be due to the small size of a cluster. We also see alot of the genre Drama. \n",
    "\n",
    "The result could be abit messy because:\n",
    "- There is no minimum on the size of a cluster when testing the heuristic\n",
    "- We set the amount of clusters to 28, because of the amount of genres. But the genres may not be even distributed and a movie may contain multiple genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list_genres = list(set(itertools.chain.from_iterable(df_nocat.genres.str.split('|'))))\n",
    "genre_percentage = list()\n",
    "\n",
    "for genre in list_genres:\n",
    "    percentage = df_nocat.loc[df_nocat['genres'].str.contains(genre)].shape[0] / df_nocat.shape[0] * 100\n",
    "    genre_percentage.append((genre, np.around(percentage, decimals=2)))\n",
    "    \n",
    "genre_percentage.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "pl_genre = [i[0] for i in genre_percentage]\n",
    "pl_percentage = [i[1] for i in genre_percentage]\n",
    "x_pos = np.arange(len(pl_genre)) \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(x_pos, pl_percentage,align='center')\n",
    "plt.xticks(x_pos, pl_genre, rotation=20) \n",
    "plt.ylabel('Ratio record occurences in percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Drama appears to be in about 50% of the dataset. The least appearing genres would be Western, Documentary and Film-Noir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_nocat.loc[df_nocat['genres'].str.contains('Western')].shape[0], 'movies with Western')\n",
    "print(df_nocat.loc[df_nocat['genres'].str.contains('Documentary')].shape[0], 'movies with Documentary')\n",
    "print(df_nocat.loc[df_nocat['genres'].str.contains('Film-Noir')].shape[0], 'movies with Film-Noir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Film-Noir only appears on 1 row. Western and Documentary stand on their own.\n",
    "\n",
    "We can try again with less clusters, combining some genre's. When testing these clusters, all of the genres within the defined genre should appear in that row. Let's cap the total combined genre's at 14. These genre's will be either combined with AND (?=.*) or OR (|) in regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_genre_list = ['(?=.*Drama)(?=.*Romance)', '(?=.*Comedy)(?=.*Romance)', '(?=.*Action)(?=.*Adventure)', 'Sport', 'Crime',\n",
    "                  '(?=.*Drama)(?=.*Thriller)', 'Horror|Mystery', '(?=.*Comedy)(?=.*Family)', '(?=.*Action)(?=.*Sci-Fi)'\n",
    "                  , 'War|History|Western', 'Animation', 'Music|Musical', 'Documentary|Biography', '(?=.*Action)(?=.*Fantasy)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to combine the higher occuring genres alot, so that it evens out. Let's see what the current distribution between the combined genre's are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "genre_percentage = list()\n",
    "\n",
    "for genre in new_genre_list:\n",
    "    percentage = df_nocat.loc[df_nocat['genres'].str.contains(genre, regex=True)].shape[0] / df_nocat.shape[0] * 100\n",
    "    genre_percentage.append((genre, np.around(percentage, decimals=2)))\n",
    "    \n",
    "genre_percentage.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "pl_genre = [i[0] for i in genre_percentage]\n",
    "pl_percentage = [i[1] for i in genre_percentage]\n",
    "x_pos = np.arange(len(pl_genre)) \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(x_pos, pl_percentage,align='center')\n",
    "plt.xticks(x_pos, pl_genre, rotation=40) \n",
    "plt.ylabel('Ratio record occurences in percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new genre balances out somewhat better than before.\n",
    "\n",
    "We need to add the new heuristic (size of cluster) and test cases (combined genre) to the function. For the size of a cluster, let's pick a minimum of 20. We also lower the percentage range from 60 to 25 to be able to print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Drop last Cluster\n",
    "df_nocat = df_nocat.iloc[:, :-1]\n",
    "\n",
    "n_cluster = 14\n",
    "heuristic_perc = 25\n",
    "chosen_genres = new_genre_list\n",
    "chosen_var = df_nocat.iloc[:,3:df_nocat.shape[1]].columns\n",
    "\n",
    "def sum_per_cluster(df, list_var, list_genres, n_clsters, heuristic_percentage):\n",
    "    print('Cluster\\tPercentage\\tGenre')\n",
    "    allMeans = KMeans(n_clusters=n_clsters, random_state=0).fit(df.loc[:,list_var])\n",
    "    df['Clusters'] = allMeans.predict(df.loc[:,list_var])\n",
    "    for cluster in range(0,n_cluster):\n",
    "        if df.loc[df['Clusters'] == cluster].shape[0] < 20:\n",
    "            continue\n",
    "        for genre in list_genres:\n",
    "            percentage = (df.loc[df['Clusters'] == cluster].genres.str.contains(genre, regex=True).sum() / df.loc[df['Clusters'] == cluster].shape[0]) * 100\n",
    "            if percentage >= heuristic_percentage:\n",
    "                print(cluster, '\\t', np.around(percentage, decimals=2), '\\t\\t', genre)\n",
    "                \n",
    "sum_per_cluster(df_nocat, chosen_var, chosen_genres, n_cluster, heuristic_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm looks like the results are even worse when combining genres, because the percentages are alot lower. How about if we pick different variables? Let's keep all social media stuff and remove properties from the movie itself (and vica versa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chosen_var = ['imdb_score', 'actor_1_facebook_likes',\n",
    "       'actor_2_facebook_likes', 'actor_3_facebook_likes',\n",
    "       'cast_total_facebook_likes', 'num_critic_for_reviews',\n",
    "       'num_user_for_reviews', 'num_voted_users']\n",
    "\n",
    "sum_per_cluster(df_nocat, chosen_var, chosen_genres, n_cluster, heuristic_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chosen_var = ['title_year', 'duration', 'aspect_ratio',\n",
    "       'facenumber_in_poster', 'budget', 'gross']\n",
    "\n",
    "sum_per_cluster(df_nocat, chosen_var, chosen_genres, n_cluster, heuristic_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the latter model we see somewhat more distinct genres. However the percentages are still not good enough to say that kMeans can cluster this set into different groups of genres. To give it a better picture, we can visualize this by plotting the largest 3 clusters and their genre distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_nocat.Clusters.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df8 = df_nocat.loc[df_nocat['Clusters'] == 8]\n",
    "df12 = df_nocat.loc[df_nocat['Clusters'] == 12]\n",
    "df3 = df_nocat.loc[df_nocat['Clusters'] == 3]\n",
    "\n",
    "genre_percentage8 = list()\n",
    "genre_percentage12 = list()\n",
    "genre_percentage3 = list()\n",
    "\n",
    "for genre in new_genre_list:\n",
    "    percentage8 = df8.loc[df8['genres'].str.contains(genre, regex=True)].shape[0] / df8.shape[0] * 100\n",
    "    percentage12 = df12.loc[df12['genres'].str.contains(genre, regex=True)].shape[0] / df12.shape[0] * 100\n",
    "    percentage3 = df3.loc[df3['genres'].str.contains(genre, regex=True)].shape[0] / df3.shape[0] * 100\n",
    "    genre_percentage8.append((genre, np.around(percentage8, decimals=2)))\n",
    "    genre_percentage12.append((genre, np.around(percentage12, decimals=2)))\n",
    "    genre_percentage3.append((genre, np.around(percentage3, decimals=2)))\n",
    "\n",
    "barwidth = 0.25    \n",
    "\n",
    "pl_genre8 = [i[0] for i in genre_percentage8]\n",
    "pl_percentage8 = [i[1] for i in genre_percentage8]\n",
    "pl_percentage12 = [i[1] for i in genre_percentage12]\n",
    "pl_percentage3 = [i[1] for i in genre_percentage3]\n",
    "x_pos8 = np.arange(len(pl_genre))\n",
    "x_pos12 = [x + barwidth for x in x_pos8]\n",
    "x_pos3 = [x + barwidth for x in x_pos12]\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(x_pos8, pl_percentage8, edgecolor='white', width=barwidth)\n",
    "plt.bar(x_pos12, pl_percentage12,edgecolor='white', width=barwidth)\n",
    "plt.bar(x_pos3, pl_percentage3, edgecolor='white', width=barwidth)\n",
    "plt.xticks([r + barwidth for r in range(len(pl_genre))], pl_genre8, rotation=40) \n",
    "plt.ylabel('Ratio record occurences in percentage')\n",
    "plt.ylim(0, 70)\n",
    "plt.legend(['8', '12', '3'], title='Clusters')\n",
    "plt.hlines(60, xmin=0, xmax=14, linestyle='dashed', colors='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't really a good distinction between the large clusters themselves, except for green maybe which outshines on War|History|Western and Documentary|Biography. **But none of the distributions are close to the 60% heuristic**. Therefor it is safe to say that clustering this dataset with kMeans **cannot** result into groups of genres.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To what extend can you predict the gross of a movie based on its popularity on Facebook and IMDB?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will determine how the gross is impacted by all the factors, then we will see what the best (most accurate) formula is.\n",
    "The factors we want to check are \n",
    "1. number of critic reviews\n",
    "2. number of user reviews\n",
    "3. movie facebook likes\n",
    "4. imdb score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "plt_scatter = scatter_matrix(df_movies[['gross', 'num_critic_for_reviews', 'num_user_for_reviews', 'imdb_score', 'movie_facebook_likes']], alpha=0.2, figsize=(9,9), diagonal='kde').view()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% clean up data for the columns we'll be using\n"
    }
   },
   "outputs": [],
   "source": [
    "df_movies.replace([np.inf, -np.inf], np.nan).dropna(subset=['num_critic_for_reviews','num_user_for_reviews', 'movie_facebook_likes', 'imdb_score','gross'], how=\"all\")\n",
    "df_movies = df_movies.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a Python list of feature names\n",
    "feature_cols = ['num_critic_for_reviews','num_user_for_reviews', 'movie_facebook_likes', 'imdb_score',]\n",
    "\n",
    "# use the list to select a subset of the original DataFrame\n",
    "X = df_movies[feature_cols]\n",
    "\n",
    "\n",
    "# print the first 5 rows\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# select a Series from the DataFrame\n",
    "y = df_movies['gross']\n",
    "\n",
    "# print the first 5 values\n",
    "y.head()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we start the model on all the different variables to create a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# fit the model to the training data (learn the coefficients)\n",
    "linreg.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll look at the metrics of our formula, and how accurate it is, we do this by looking at the intercept and\n",
    "coefficient, and calculate the root mean squared error with the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)\n",
    "list(zip(feature_cols, linreg.coef_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = linreg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', linreg.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mse)\n",
    "print('Root mean squared error: %.2f'\n",
    "      % np.math.sqrt(mse))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not good, a value of close to 1 is ideal, this is extremely high. We'll drop the columns that seem to have the least correlation according to the scatter plot.\n",
    "We will drop number of critics for review as it seems to have the least correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a Python list of feature names\n",
    "feature_cols_2 = ['movie_facebook_likes', 'imdb_score','num_user_for_reviews']\n",
    "\n",
    "# use the list to select a subset of the original DataFrame\n",
    "X_2 = df_movies[feature_cols_2]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y, random_state=0)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate\n",
    "linreg_2 = LinearRegression()\n",
    "\n",
    "# fit the model to the training data (learn the coefficients)\n",
    "linreg_2.fit(X_train_2, y_train_2)\n",
    "\n",
    "print(linreg_2.intercept_)\n",
    "print(linreg_2.coef_)\n",
    "list(zip(feature_cols_2, linreg_2.coef_))\n",
    "\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred_2 = linreg_2.predict(X_test_2)\n",
    "mse = mean_squared_error(y_test_2, y_pred_2)\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', linreg_2.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mse)\n",
    "print('Root mean squared error: %.2f'\n",
    "      % np.math.sqrt(mse))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y_test_2, y_pred_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a good result, we want something not very far from 0, it even went up a slight bit. Let's drop even more columns, this time we'll remove number of user reviews and look again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a Python list of feature names\n",
    "feature_cols_3 = ['movie_facebook_likes', 'imdb_score',]\n",
    "\n",
    "# use the list to select a subset of the original DataFrame\n",
    "X_3 = df_movies[feature_cols_3]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_3, y, random_state=0)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate\n",
    "linreg_3 = LinearRegression()\n",
    "\n",
    "# fit the model to the training data (learn the coefficients)\n",
    "linreg_3.fit(X_train_3, y_train_3)\n",
    "\n",
    "print(linreg_3.intercept_)\n",
    "print(linreg_3.coef_)\n",
    "list(zip(feature_cols_3, linreg_3.coef_))\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred_3 = linreg_3.predict(X_test_3)\n",
    "mse = mean_squared_error(y_test_3, y_pred_3)\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', linreg_3.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mse)\n",
    "print('Root mean squared error: %.2f'\n",
    "      % np.math.sqrt(mse))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y_test_3, y_pred_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems dropping columns is not the answer, our first prediction is still the best, but it's definitely a large margin of error.\n",
    "It would seem that the correlation is weak at best, and removing columns is not a solution to the problem. Popularity on IMDB and facebook seems like\n",
    "a weak indicator for how well a movie will do financially.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z-toets IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Een filmcriticus stelt dat de score van engelstalige films lager is dan gemiddeld.\n",
    "\n",
    "Onderzoek met de dataset of deze filmcriticus gelijk heeft. Neem een steekproef (met ```pandas.DataFrame.sample(n=100,random_state=1)```) van 100 engelstalige films en beschouw de hele dataset als populatie. Neem als betrouwbaarheid 90%. Gebruik van de dataset alleen de filmgegevens waarbij zowel de taal (`language`) als de score (`imdb_score`) bekend zijn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv('movie.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_movies = movies[movies.imdb_score.notnull()]\n",
    "all_movies = all_movies[all_movies.language.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "movies_english = all_movies[all_movies.language == 'English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample = movies_english.sample(n=100,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample.boxplot(column=\"imdb_score\")\n",
    "plt.show()\n",
    "all_movies.boxplot(column=\"imdb_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample.imdb_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_movies.imdb_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stdev_en = st.tstd(sample[\"imdb_score\"])\n",
    "\n",
    "print(stdev_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the accuracy of our findings we have to do a Z-test. We will set out our hypothesis and null hypothesis and test the latter.\n",
    "\n",
    "These are as follows:\n",
    "\n",
    "H0 = English films score as well or better than other movies on IMDB. μother <= μenglish = 6.35\n",
    "\n",
    "H1 = English films score significantly worse than other movies on IMDB. μother > μenglish = 6.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "good_score = movies_english[movies_english.imdb_score >= all_movies.imdb_score.mean()].count()\n",
    "q = .5\n",
    "z_alpha = 1.29\n",
    "mean_english_score = sample.imdb_score.mean()\n",
    "mean_score = all_movies.imdb_score.mean()\n",
    "\n",
    "se = stdev_en / (np.sqrt(n))\n",
    "\n",
    "z = (mean_score - mean_english_score) / se\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The z value we found from the calculation is 0.68, this is significantly lower than the value we'd want of 1.29 or higher, \n",
    "we can therefore not reject the nul hypothesis, and not prove the alternative hypothesis either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The z value we found from the calculation is 0.68, this is significantly lower than the value we'd want of 1.29 or higher, \n",
    "we can therefore not reject the nul hypothesis, and not prove the alternative hypothesis either."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}